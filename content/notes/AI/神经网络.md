---
title: 神经网络
date: 2025-06-30
details: 核心网络架构和常用激活函数总结
---
# 神经网络

## 1. 核心网络架构总结 (MLP, CNN, RNN)

不同的网络架构是为解决不同类型的问题而设计的。它们的核心区别在于如何处理和“看待”数据。

| 架构 | **MLP (多层感知机)** | **CNN (卷积神经网络)** | **RNN (循环神经网络)** |
| :--- | :--- | :--- | :--- |
| **核心思想** | 通用的“万能”函数拟合器 | 专为网格/空间数据设计的“特征提取器” | 专为序列/时间数据设计的“记忆网络” |
| **关键模块** | **全连接层 (Dense Layer)** | **卷积层 (Conv Layer)**, **池化层 (Pooling Layer)** | **循环单元 (Recurrent Cell)** (如LSTM/GRU) |
| **数据处理方式** | 将数据视为**扁平的向量**，不考虑其内在结构 | 将数据视为**二维或三维网格**，利用其空间局部性 | 将数据视为**有时间顺序的序列**，处理时序依赖 |
| **最擅长领域** | **结构化/表格数据**（如Excel表） | **图像、视频、棋盘游戏** | **文本、语音、时间序列数据**（如股票） |
| **解决问题示例** | 信用评分、房价预测 | 图像分类、人脸识别、自动驾驶 | 机器翻译、情感分析、语音识别 |
| **类比** | **全民投票系统** | **分工明确的视觉检查流水线** | **有短期记忆的阅读者** |

### A. MLP (Multilayer Perceptron) - 全连接神经网络

* **工作方式**:
    1.  **结构**: 由多个“全连接层”堆叠而成。在一个全连接层中，**每一个**神经元都与**上一层的所有**神经元相连接。
    2.  **数据流**: 数据以一个**一维向量**的形式输入（例如，一张 `28x28` 的图片会被展平成 `784` 个元素的向量）。数据从输入层单向地流向输出层，每经过一层，就进行一次“矩阵乘法 + 激活函数”的计算。
    3.  **算法**: `output = activation(input @ W + b)`，其中 `W` 是权重矩阵，`b` 是偏置。
* **优缺点**:
    * **优点**: 结构简单，理论上可以拟合任何函数。
    * **缺点**: 当输入数据维度很高时（如高分辨率图像），权重参数数量会**爆炸式增长**，导致计算效率低下且容易过拟合。它也完全**忽略了数据的空间结构**（比如图片中像素与像素之间的邻里关系）。

### B. CNN (Convolutional Neural Network) - 卷积神经网络

* **工作方式**:
    1.  **结构**: 核心是**卷积层**和**池化层**。
    2.  **卷积层**: 使用一个小的**卷积核 (Kernel/Filter)**，像一个“手电筒”一样在输入的二维/三维数据上滑动。这个卷积核是一个小权重矩阵，专门用来检测某种局部特征（如边缘、角落、颜色块）。这个过程有两大特点：
        * **局部感受野 (Local Receptive Fields)**：每个神经元只看输入的一小块区域，而不是全部。
        * **参数共享 (Parameter Sharing)**：同一个卷积核在整张图上共享同一套权重，大大减少了参数量。
    3.  **池化层 (Pooling)**：在卷积后进行，目的是**下采样**。它将一个小区域内的特征压缩成一个值（如取最大值的Max Pooling），这可以降低数据维度，并使模型对特征的微小位移不那么敏感（平移不变性）。
    4.  **算法**: 通过堆叠“卷积-激活-池化”模块，CNN能从底层（边缘、颜色）到高层（物体部件、完整物体）**逐层提取越来越复杂的特征**。最后通常会接上几个全连接层（MLP）进行分类或回归。

### C. RNN (Recurrent Neural Network) - 循环神经网络

* **工作方式**:
    1.  **结构**: 其核心特点是神经元之间存在**循环连接**。
    2.  **数据流**: RNN按时间步处理数据。在 `t` 时刻，神经元的输入不仅包括当前时刻的数据 `x_t`，还包括**上一时刻 `t-1` 的隐藏状态 `h_{t-1}`**。
    3.  **算法**: `h_t = activation(W_hh @ h_{t-1} + W_xh @ x_t + b)`。这个**隐藏状态 `h_t`** 就相当于网络的“记忆”，它包含了过去所有时间步的信息摘要。
* **优缺点**:
    * **优点**: 能很好地处理序列数据，捕捉时间依赖关系。
    * **缺点**: 传统的RNN有**梯度消失/爆炸**问题，导致它很难学习到“长期依赖”（例如，一篇文章开头的信息对结尾很重要）。
* **重要变体**:
    * **LSTM (长短期记忆网络)** 和 **GRU (门控循环单元)** 是RNN的改进版。它们引入了精巧的“**门控机制**”（遗忘门、输入门、输出门），让网络可以有选择性地学习**哪些信息要长期保留，哪些信息可以遗忘**，从而极大地解决了长期依赖问题，是目前处理序列任务的主流选择。

## 2. 常用激活函数总结

**为什么需要激活函数？**
激活函数的目的是向网络中引入**非线性**。如果没有激活函数，无论神经网络有多少层，其本质上都只是一个复杂的线性模型，无法学习和拟合复杂的数据模式。

| 激活函数 (Activation Function) | 公式 (Formula) | 输出范围 (Output Range) | 优点 (Pros) | 缺点 (Cons) | 主要用途 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Sigmoid** | $\frac{1}{1 + e^{-x}}$ | (0, 1) | 输出可解释为概率，平滑 | **梯度消失严重**，非零中心 | 二分类任务的输出层 |
| **Tanh** | $\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | (-1, 1) | **零中心**，收敛速度通常比Sigmoid快 | 梯度消失问题依然存在 | 可用于隐藏层，但已较少使用 |
| **ReLU** | $\max(0, x)$ | [0, +∞) | **从根本上解决梯度消失** (正区间)，计算非常快，引入稀疏性 | **可能“死亡”** (Dying ReLU)，非零中心 | **现代CNN/MLP隐藏层的默认首选** |
| **Leaky ReLU** | $\max(\alpha x, x)$ (α很小) | (-∞, +∞) | **解决了“死亡ReLU”问题**，保留ReLU大部分优点 | 效果不总比ReLU好，需要调超参α | 当ReLU效果不佳时的替代方案 |
| **Softmax** | $\frac{e^{x_i}}{\sum_{j} e^{x_j}}$ | (0, 1)，且所有输出之和为1 | 输出为**概率分布**，清晰明了 | 计算相对复杂 | **多分类任务的输出层（唯一选择）** |

### 各激活函数详解

* **Sigmoid**: 历史悠久，但因其在输入值很大或很小时导数趋近于0，在深层网络中会导致梯度消失，现已很少用于隐藏层。
* **Tanh (双曲正切)**: 可以看作是Sigmoid的“零中心”版本，性能通常优于Sigmoid，但同样有梯度消失问题。
* **ReLU (修正线性单元)**: **现代神经网络的基石**。当输入为正时，导数恒为1，梯度可以顺畅地在网络中流动。其最大的问题是“死亡ReLU”：如果一个神经元的输入恒为负，它的权重将永远无法更新。
* **Leaky ReLU (带泄露的ReLU)**: 是对ReLU的改进。它给负输入值一个非常小的正斜率（如0.01），确保神经元在任何时候都有梯度，从而不会“死亡”。
* **Softmax**: 它不是一个通用的隐藏层激活函数，而是专门用于**多分类问题的输出层**。它能确保所有输出值的和为1，完美地对应了“样本属于各个类别的概率”。