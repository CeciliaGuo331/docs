---
title: 1_从人工智能到大模型
date: 2025-06-30
details: 导论课
---
# 1_从人工智能到大模型

## 人工智能与大模型发展史

见[深度学习原理与PyTorch实战](https://ceciliaguo331.github.io/docs/content/note/%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%9A%E4%BB%8E%E5%9F%BA%E7%A1%80%E5%88%B0%E5%AE%9E%E6%88%98/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%9F%E7%90%86%E4%B8%8EPyTorch%E5%AE%9E%E6%88%98.html)

## 大模型科学探索

1. 维数灾难？

    - “维数灾难”：传统计算理论认为，高维空间计算复杂度会指数增长，计算时间会随参数量的增加而指数攀升（NP问题），习惯于降维。

    - “维数福音”：深度神经网络表现出不同的性质：训练时间仅与参数数量呈线性增长。一些困难的问题反而会随着空间维度的增加而变得更容易解决。

    - 与传统方法相比，机器学习解决的最基本的问题就是函数的表达和逼近。数学上有分片多项式、傅利叶级数、小波这都是传统的表达函数的套路，只能处理低维问题。深度学习解决的许多问题都是非常高维的，正确的方法应该是看一个高维函数是否能被一个特定的像神经网络这样的模型来有效地逼近。

2. 陷入局部最小？

    - 事实：神经网络模型能够高效地寻找到解决方案，并实现良好的泛化

    - 可能原因：

        - 高维空间坑少，鞍点多，落入全局最小并不难

        - 高维空间存在大量“准最优解”

3. 参数多过拟合？

    - 按照经典统计学的预测，当网络参数规模足够大时，训练误差最终会接近于零，而测试误差会因模型过拟合而开始上升。

    - 但大模型实践表明，在更大规模的网络模型中，测试误差会出现拐点并继续下降，展现出优秀的泛化能力，与过拟合恰恰相反。